{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Graph Neural Network Tracker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "from nb_utils import (compute_metrics, plot_metrics, draw_sample_xy, draw_sample, load_summaries)\n",
    "\n",
    "from xtracker.gnn_tracking.ImTracker import ImTracker \n",
    "from xtracker.gnn_tracking.TrackingSolver import TrackingSolver\n",
    "from xtracker.gnn_tracking.TrackingGame import TrackingGame as Game\n",
    "from xtracker.gnn_tracking.pytorch.NNet import NNetWrapper \n",
    "from xtracker.utils import dotdict\n",
    "from itertools import cycle\n",
    "from xtracker.datasets import get_data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit CPU usage on Jupyter\n",
    "os.environ['OMP_NUM_THREADS'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./../examples/configs/belle2_vtx.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "data_args = dotdict( config['data'] )\n",
    "train_data_loader, valid_data_loader = get_data_loaders(**data_args,  input_dir=config['global']['graph_dir'])\n",
    "assert valid_data_loader is not None\n",
    "assert train_data_loader is not None\n",
    "\n",
    "valid_data_loader = cycle(valid_data_loader)\n",
    "train_data_loader = cycle(train_data_loader)\n",
    "\n",
    "\n",
    "game = Game(train_data_loader, valid_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length scales for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_phi_sections = 1\n",
    "feature_scale_r = config['selection']['feature_scale_r']\n",
    "feature_scale_phi = config['selection']['feature_scale_phi']\n",
    "feature_scale_z = config['selection']['feature_scale_z']\n",
    "\n",
    "feature_scale = np.array([feature_scale_r, np.pi / n_phi_sections / feature_scale_phi, feature_scale_z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained neural net\n",
    "n1 = NNetWrapper()\n",
    "checkpoint_dir = os.path.expandvars(config['training']['checkpoint'])\n",
    "n1.load_checkpoint(checkpoint_dir, 'best.pth.tar')\n",
    "\n",
    "# Built a tracker\n",
    "tracker_args = dotdict(config['model'])\n",
    "tracker = ImTracker(game, n1, tracker_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = load_summaries(checkpoint_dir)\n",
    "\n",
    "print('\\nTraining summaries:')\n",
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best epoch\n",
    "best_idx = summaries.pit_nnet_score.idxmax()\n",
    "summaries.loc[[best_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcomment this line to use the mc solution\n",
    "\n",
    "#tracker =  TrackingSolver(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate tracker on individual events "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "graph = next(valid_data_loader)  \n",
    "board = game.getInitBoardFromBatch(graph)\n",
    "\n",
    "\n",
    "pred, score, trig = tracker.process(board)   \n",
    "print('score=', score)\n",
    "print('pred trigger ', trig)\n",
    "print('true trigger ', board.trig)\n",
    "\n",
    "draw_sample_xy(board.x * feature_scale, board.edge_index, pred, board.y, cut=0.5, mconly=False, fullonly=False,\n",
    "              figsize=(9, 9)\n",
    ")\n",
    "draw_sample(board.x * feature_scale, board.edge_index, pred, board.y, cut=0.5, mconly=False, fullonly=False, \n",
    "           figsize=(9, 6))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    graph = next(valid_data_loader)  \n",
    "    board = game.getInitBoardFromBatch(graph)\n",
    "\n",
    "    pred, score, trig = tracker.process(board)   \n",
    "    print('score=', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate tracker with statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(data_loader, game, tracker, verbose=False, n=12):\n",
    "    \n",
    "    preds, targets = [], []\n",
    "    i = 0\n",
    "   \n",
    "    for _ in range(n):\n",
    "      \n",
    "        graph = next(data_loader)  \n",
    "        board = game.getInitBoardFromBatch(graph)\n",
    "        \n",
    "        pred, score, trig = tracker.process(board)   \n",
    "         \n",
    "        if verbose:     \n",
    "            test_metrics = compute_metrics([pred], [board.y], threshold=0.5)\n",
    "            print('Accuracy:  %.4f' % test_metrics.accuracy)\n",
    "            print('Precision: %.4f' % test_metrics.precision)\n",
    "            print('Recall:    %.4f' % test_metrics.recall)    \n",
    "            \n",
    "                \n",
    "        preds.append(pred)\n",
    "        targets.append(board.y)\n",
    "        i = i + 1\n",
    "    return preds, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Apply the model\n",
    "test_preds, test_targets = predict_sample(valid_data_loader, game=game, tracker=tracker,\n",
    "                                                              verbose=False, n=32) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5 \n",
    "test_metrics = compute_metrics(test_preds, test_targets, threshold=threshold)\n",
    "\n",
    "print('Faster Test set results with threshold of', threshold)\n",
    "print('Accuracy:  %.4f' % test_metrics.accuracy)\n",
    "print('Precision: %.4f' % test_metrics.precision)\n",
    "print('Recall:    %.4f' % test_metrics.recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Belle2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
